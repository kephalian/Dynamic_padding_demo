# -*- coding: utf-8 -*-
"""Processing the data (PyTorch)

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/huggingface/notebooks/blob/master/course/en/chapter3/section2_pt.ipynb

# Processing the data (PyTorch)

Install the Transformers, Datasets, and Evaluate libraries to run this notebook.
"""

!pip install datasets evaluate transformers[sentencepiece]

import torch
from transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification

# Same as before
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
sequences = [
    "I've been waiting for a HuggingFace course my whole life.",
    "This course is amazing!",
]
batch = tokenizer(sequences, padding=True, truncation=True, return_tensors="pt")

# This is new
batch["labels"] = torch.tensor([1, 1])

optimizer = torch.optim.AdamW(model.parameters())
#AdamW(model.parameters())
loss = model(**batch).loss
loss.backward()
optimizer.step()

from datasets import load_dataset

raw_datasets = load_dataset("glue", "mrpc")
raw_datasets

"""As you can see, we get a DatasetDict object which contains the training set, the validation set, and the test set. Each of those contains several columns (sentence1, sentence2, label, and idx) and a variable number of rows, which are the number of elements in each set (so, there are 3,668 pairs of sentences in the training set, 408 in the validation set, and 1,725 in the test set).

This command downloads and caches the dataset, by default in ~/.cache/huggingface/datasets. Recall from Chapter 2 that you can customize your cache folder by setting the HF_HOME environment variable.

We can access each pair of sentences in our raw_datasets object by indexing, like with a dictionary:
"""

raw_train_dataset = raw_datasets["train"]
raw_train_dataset[1]

"""We can see the labels are already integers, so we wonâ€™t have to do any preprocessing there. To know which integer corresponds to which label, we can inspect the features of our raw_train_dataset. This will tell us the type of each column:"""

raw_train_dataset.features

"""To preprocess the dataset, we need to convert the text to numbers the model can make sense of. As you saw in the previous chapter, this is done with a tokenizer. We can feed the tokenizer one sentence or a list of sentences, so we can directly tokenize all the first sentences and all the second sentences of each pair like this:"""

from transformers import AutoTokenizer

checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
tokenized_sentences_1 = tokenizer(raw_datasets["train"]["sentence1"])
tokenized_sentences_2 = tokenizer(raw_datasets["train"]["sentence2"])

inputs = tokenizer("This is the first sentence.", "This is the second one.")
inputs

"""If we decode the IDs inside input_ids back to words:"""

tokenizer.convert_ids_to_tokens(inputs["input_ids"])

"""Now that we have seen how our tokenizer can deal with one pair of sentences, we can use it to tokenize our whole dataset: like in the previous chapter, we can feed the tokenizer a list of pairs of sentences by giving it the list of first sentences, then the list of second sentences. This is also compatible with the padding and truncation options we saw in Chapter 2. So, one way to preprocess the training dataset is"""

tokenized_dataset = tokenizer(
    raw_datasets["train"]["sentence1"],
    raw_datasets["train"]["sentence2"],
    padding=True,
    truncation=True,
)

"""To keep the data as a dataset, we will use the Dataset.map() method. This also allows us some extra flexibility, if we need more preprocessing done than just tokenization. The map() method works by applying a function on each element of the dataset, so letâ€™s define a function that tokenizes our inputs:"""

def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

"""Here is how we apply the tokenization function on all our datasets at once. Weâ€™re using batched=True in our call to map so the function is applied to multiple elements of our dataset at once, and not on each element separately. This allows for faster preprocessing."""

tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)
tokenized_datasets

"""The way the ðŸ¤— Datasets library applies this processing is by adding new fields to the datasets, one for each key in the dictionary returned by the preprocessing function:

To do this in practice, we have to define a collate function that will apply the correct amount of padding to the items of the dataset we want to batch together. Fortunately, the ðŸ¤— Transformers library provides us with such a function via DataCollatorWithPadding. It takes a tokenizer when you instantiate it (to know which padding token to use, and whether the model expects padding to be on the left or on the right of the inputs) and will do everything you need:
"""

from transformers import DataCollatorWithPadding

data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

"""To test this new toy, letâ€™s grab a few samples from our training set that we would like to batch together. Here, we remove the columns idx, sentence1, and sentence2 as they wonâ€™t be needed and contain strings (and we canâ€™t create tensors with strings) and have a look at the lengths of each entry in the batch:"""

samples = tokenized_datasets["train"][-20:-2]
samples = {k: v for k, v in samples.items() if k not in ["idx", "sentence1", "sentence2"]}
[len(x) for x in samples["input_ids"]]

"""No surprise, we get samples of varying length, from 36 to 71. Dynamic padding means the samples in this batch should all be padded to a length of 71, the maximum length inside the batch. Without dynamic padding, all of the samples would have to be padded to the maximum length in the whole dataset, or the maximum length the model can accept. Letâ€™s double-check that our data_collator is dynamically padding the batch properly:"""

batch = data_collator(samples)
{k: v.shape for k, v in batch.items()}

"""Replicate the preprocessing on the GLUE SST-2 dataset. Itâ€™s a little bit different since itâ€™s composed of single sentences instead of pairs, but the rest of what we did should look the same. For a harder challenge, try to write a preprocessing function that works on any of the GLUE tasks."""

from datasets import load_dataset

raw_datasets = load_dataset("glue", "sst2")
raw_datasets

raw_train_dataset = raw_datasets["train"]
raw_train_dataset[-1]

def tokenize_function2(example):
    return tokenizer(example["sentence"], truncation=True)

tokenized_datasets2 = raw_datasets.map(tokenize_function2, batched=True)
tokenized_datasets

samples2 = tokenized_datasets2["train"][0:67348]
 #[-20:-2]
samples2 = {k: v for k, v in samples2.items() if k not in ["idx", "sentence",]}
i=[len(x) for x in samples2["input_ids"]]
import numpy as np
from scipy import stats as st
#np.dim(i)
print("*"*50)
print(f"Number of enteries in Training is {len(i)}")
print(f"**** Maximum length {np.max(i)}  Minimum length {np.min(i)}******")
print(f"Mean is {np.round(np.mean(i),2)} Median is {np.median(i)} Mode is {st.mode(i).mode}")
print("*"*50)

from transformers import DataCollatorWithPadding

data_collator2 = DataCollatorWithPadding(tokenizer=tokenizer)

batch = data_collator2(samples2)
{k: v.shape for k, v in batch.items()}

"""# Dyanamic padding has made all sentence uniform to maximum size of the sentence in the dataset i.e 66 See previous cell for maximum size"""